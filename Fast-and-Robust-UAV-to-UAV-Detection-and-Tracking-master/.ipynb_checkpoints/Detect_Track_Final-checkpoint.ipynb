{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import operator\n",
    "import os\n",
    "from numpy import zeros, newaxis\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import skimage\n",
    "import skimage.io\n",
    "import scipy.io as scp\n",
    "from sklearn.utils import shuffle\n",
    "from __future__ import print_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Generate_pm_pa import *\n",
    "from UAV_subfunctions import *\n",
    "from Extract_Patch import *\n",
    "from Detect_Patch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/cent7/anaconda/5.1.0-py36-new/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.externals import joblib\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ReduceLROnPlateau,EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = '/scratch/gilbreth/li1463/UAV_Project/Data/'\n",
    "app_model_path = '/scratch/gilbreth/li1463/UAV_Project/models/max500_1_10_threelayers/'\n",
    "app_model_path_track = '/scratch/gilbreth/li1463/UAV_Project/models/Appearance_OriImage/'\n",
    "\n",
    "mvmodel_path = '/scratch/gilbreth/li1463/UAV_Project/models/motion/200/'\n",
    "bimodel_path = '/scratch/gilbreth/li1463/UAV_Project/models/Adaboost/'\n",
    "\n",
    "bimodel_path_track = '/scratch/gilbreth/li1463/UAV_Project/models/Adaboost_track/Adaboost_track/'\n",
    "#defult is 15, 2\n",
    "trackwinS =15\n",
    "\n",
    "video_savePath_features  = '/scratch/gilbreth/li1463/UAV_Project/Experiment_Results/Final/Video/'\n",
    "    \n",
    "if not os.path.exists(video_savePath_features):\n",
    "    print (\"path doesn't exist. trying to make\")\n",
    "    os.makedirs(video_savePath_features)\n",
    "    \n",
    "video_savePath_Detection  =  '/scratch/gilbreth/li1463/UAV_Project/Experiment_Results/Final/txt/'\n",
    "    \n",
    "if not os.path.exists(video_savePath_Detection):\n",
    "    print (\"path doesn't exist. trying to make\")\n",
    "    os.makedirs(video_savePath_Detection)\n",
    "par = []\n",
    "par.append([0.001,40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "complete shuffling\n",
      "[13, 39, 30, 45, 17, 48, 26, 25, 32, 19, 12, 4, 37, 8, 3, 6, 41, 46, 47, 15, 9, 16, 24, 34, 31, 0, 44, 27, 33, 5, 29, 11, 36, 1, 21, 2, 43, 35, 23, 40, 10, 22, 18, 49, 20, 7, 42, 14, 28, 38]\n"
     ]
    }
   ],
   "source": [
    "index= list(range(50))\n",
    "print(index)\n",
    "from sklearn.utils import shuffle\n",
    "index = shuffle(index, random_state=42)\n",
    "print(\"complete shuffling\")\n",
    "print(index)\n",
    "maxD=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 38, 38, 16)        880       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 36, 36, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 548,690\n",
      "Trainable params: 548,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 38, 38, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 36, 36, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 548,258\n",
      "Trainable params: 548,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 818\n",
      "Trainable params: 818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Video: 14\n",
      "1000 0.001 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-72ba979ca721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframeidx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdetect_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;31m#p_pos, p_pos_errImg, p_neg, p_neg_errImg, p_pos_gt, p_pos_gt_errImg, hit,ftNo,FAno, vis_points = generatePatches(frameidx, gray, Xt, weightedError, centers, H_back, feature_params_track, feature_params_track_oriImg, lk_params_track, radius, color, future_color, gt_mask, oriImage)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetectedPatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorPatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetectedLocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurLocslll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftNo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFAno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtract_Patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_params_track\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_params_track_oriImg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk_params_track\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;31m#errorPatches, detectedPatches, detectedLocs, Locs_next, Locs_pers, d_match = generatePatches_online(gray, Xt, weightedError, feature_params_track, color, radius,lk_params_track,H_back)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gilbreth/li1463/UAV_Project/Fast-and-Robust-UAV-to-UAV-Detection-and-Tracking/Extract_Patch.py\u001b[0m in \u001b[0;36mExtract_Patch\u001b[0;34m(frameidx, gray, Xt, weightedError, centers, H_back, ftparmes, ftparmes_ori, lk_params_track, radius, Xt_1, Xt_color, gt_mask, gt_img)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mweightedError\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mweightedError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfeaturesforBackgroundSubtractedImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_GRAY2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoodFeaturesToTrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m**\u001b[0m\u001b[0mftparmes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mpall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ind in range(1,2):\n",
    "    appmodel=load_model(app_model_path+str(ind)+'.h5')\n",
    "    appmodel.summary()\n",
    "\n",
    "    appmodel_track=load_model(app_model_path_track+str(ind)+'.h5')\n",
    "    appmodel_track.summary()\n",
    "    \n",
    "    mvmodel = load_model(mvmodel_path+str(ind)+'.h5')\n",
    "    mvmodel.summary()\n",
    "    \n",
    "    combinemodel = joblib.load(bimodel_path+'fold'+str(ind)+'.pkl')\n",
    "    \n",
    "    combinemodel_track = joblib.load(bimodel_path_track+'fold'+str(ind)+'.pkl')\n",
    "    a = 0.001\n",
    "    b = 50\n",
    "    #test_ind = index[10*(ind-1):10*(ind-1)+10]\n",
    "    test_ind = index[10*(ind-1):10*(ind-1)+10]\n",
    "    objNo = 0\n",
    "    dtNo = 0\n",
    "    htNo = 0\n",
    "    allFA = 0\n",
    "    for i in test_ind:\n",
    "        all_params = dict(videoName = str(i+1),\n",
    "                          downrightx = 350,\n",
    "                          upleftx = 0,\n",
    "                          downrighty = 780,\n",
    "                          uplefty = 510,\n",
    "                          fileName = 'supervised_SVM_'+str(i),\n",
    "                          debug = 1,\n",
    "                          qualityini = 0.005,#np.float32(sys.argv[4]),#1.5#,\n",
    "                          K = 1,# number of previous frames before Xt-1\n",
    "                          MaxCorns = 600,#np.int16(sys.argv[5]),#200,#600/(resizeFactor*resizeFactor),\n",
    "                          mindist1 = 25,#np.int16(sys.argv[6]),#15,\n",
    "                          quality = a,#np.float32(sys.argv[7]),#0.001 #,\n",
    "                          maxcorners = 1000,#np.int16(sys.argv[8]),#100,#/(resizeFactor*resizeFactor),\n",
    "                          mindist = b,#15,#np.int16(sys.argv[9]),#1,\n",
    "                          use_ransac = True,\n",
    "                          track_len = 10,# track_len: maximum number of points recorded in the track\n",
    "                          lamda = 0,#taking average if bidirectional error\n",
    "                          detect_interval = 6,\n",
    "                         )\n",
    "        \n",
    "\n",
    "        print ('Video:',all_params['videoName'])\n",
    "        videoName = all_params['videoName']\n",
    "        uplefty = all_params['uplefty']\n",
    "        downrighty = all_params['downrighty']\n",
    "        upleftx = all_params['upleftx']\n",
    "        downrightx = all_params['downrightx']\n",
    "        fileName = all_params['fileName']\n",
    "        debug = all_params['debug']\n",
    "        K = all_params['K']\n",
    "        qualityini = all_params['qualityini']\n",
    "        MaxCorns = all_params['MaxCorns']\n",
    "        mindist1 = all_params['mindist1']\n",
    "        use_ransac = all_params['use_ransac']\n",
    "        track_len = all_params['track_len']\n",
    "        lamda = all_params['lamda']\n",
    "        quality = all_params['quality']\n",
    "        maxcorners = all_params['maxcorners']\n",
    "        mindist = all_params['mindist']\n",
    "        detect_interval = all_params['detect_interval']\n",
    "\n",
    "        \n",
    "\n",
    "        ## parameter for feature detection(original image) and tracking[for background subtraction]\n",
    "        feature_params = dict( maxCorners = MaxCorns,\n",
    "                          qualityLevel = qualityini,\n",
    "                          minDistance = mindist1,\n",
    "                          blockSize = 5 )\n",
    "        ## parameter for feature tracking(original image)\n",
    "        lk_params = dict( winSize  = (19, 19),\n",
    "                     maxLevel = 2,\n",
    "                     criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.03))\n",
    "\n",
    "        ## parameter for feature detection(error image) and tracking[for feature extraction and tracking]\n",
    "        print (maxcorners, quality, mindist)\n",
    "        feature_params_track = dict( maxCorners = 500,\n",
    "                                qualityLevel = quality/20.0,\n",
    "                                minDistance = mindist,\n",
    "                                blockSize = 9 )\n",
    "        \n",
    "        feature_params_track_oriImg = feature_params_track\n",
    "                                \n",
    "        lk_params_track = dict( winSize  = (19, 19),\n",
    "                     maxLevel = 2,\n",
    "                     criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.03),\n",
    "                     minEigThreshold=1e-4)\n",
    "        \n",
    "        lk_params_track_ori = dict( winSize  = (25, 25),\n",
    "                     maxLevel = 3,\n",
    "                     flags = cv2.OPTFLOW_USE_INITIAL_FLOW,\n",
    "                     criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.03),\n",
    "                     minEigThreshold=1e-4)\n",
    "\n",
    "        feature_params_Detect = dict( maxCorners = 10,\n",
    "                                qualityLevel = 0.00000015,\n",
    "                                minDistance = 0,\n",
    "                                blockSize = 3 )\n",
    "\n",
    "        #cam_gt=cv2.VideoCapture(videoPath+ 'shor_clip_gtVideo/uav_Video_'+videoName+'_gt.mov')\n",
    "        cam=cv2.VideoCapture(videoPath+ '50ClipsFromOriVideo/Clip_'+videoName+'.mov')\n",
    "        gt_text = open(videoPath+ 'Annotation_update_180925/Video_'+videoName+'_gt.txt',\"r\")\n",
    "        f_txt = open(video_savePath_Detection+ videoName+'_dt.txt','w')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # read in one frame in order to obtain the video size\n",
    "        frameidx = 1\n",
    "        color=cam.read()[1]\n",
    "        color_gt=color.copy()#cam_gt.read()[1]\n",
    "        prepreFrame = np.float32(cv2.cvtColor(color, cv2.COLOR_RGB2GRAY))\n",
    "        h,w,channel = color.shape\n",
    "        groundtruth = gt_text.readline()\n",
    "        \n",
    "        outputFeature = \"time_layer: \"+ str(frameidx)+\" detections: \"\n",
    "        f_txt.write(outputFeature+\"\\n\")\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        FPS = cam.get(cv2.CAP_PROP_FPS)        \n",
    "        video_PosPatch = cv2.VideoWriter(video_savePath_features+videoName+'.mov', fourcc,FPS,(w,h))         \n",
    "        \n",
    "            # initialize feature points\n",
    "        pImg = None\n",
    "\n",
    "\n",
    "        #initialize H_back\n",
    "        H_back = None\n",
    "\n",
    "\n",
    "    \n",
    "        # read in Xt-1\n",
    "        color=cam.read()[1]\n",
    "        color_gt =color.copy()#cam_gt.read()[1]\n",
    "        groundtruth = gt_text.readline()   \n",
    "        frameidx+=1\n",
    "        outputFeature = \"time_layer: \"+ str(frameidx)+\" detections: \"\n",
    "        #f_txt.write(outputFeature+\"\\n\")\n",
    "\n",
    "        Xtminus1 = np.float32(cv2.cvtColor(color, cv2.COLOR_RGB2GRAY))\n",
    "\n",
    "        # blocks is 1 except for the pitot tube region(0)\n",
    "        blocks = np.ones((h,w), dtype='float32')\n",
    "        #blocks[uplefty:downrighty,upleftx:downrightx ] = 0\n",
    "        # parameter for groundtruth dilation\n",
    "        dt_d = 4\n",
    "        radius = 10\n",
    "\n",
    "        Dotft = []\n",
    "        Patchft=[]\n",
    "        maxPatchId = 0\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            #print(frameidx)\n",
    "            ##############Start Detection Part############\n",
    "            ######Background Subtraction #################\n",
    "\n",
    "            gray = Xtminus1.copy()\n",
    "            # read in current frame Xt\n",
    "            future_color = cam.read()[1]\n",
    "            if future_color is None:\n",
    "                frameidx+=1\n",
    "                outputFeature = \"time_layer: \"+ str(frameidx)+\" detections: \"\n",
    "                f_txt.write(outputFeature)\n",
    "                break\n",
    "      \n",
    "            frameidx+=1\n",
    "\n",
    "            Xt = np.float32(cv2.cvtColor(future_color, cv2.COLOR_RGB2GRAY))\n",
    "\n",
    "            ## generate groundtruth mask\n",
    "            gt_split = groundtruth.split()\n",
    "            length = len(gt_split)\n",
    "            gt_index = 3\n",
    "            gt_mask = np.zeros_like(Xt)\n",
    "            gt_ft_maske = np.zeros_like(Xt)\n",
    "            bbox_index = 0\n",
    "            centers = []\n",
    "            \n",
    "            color_gt = color.copy()\n",
    "            while gt_index< length:\n",
    "\n",
    "                bbox_index+=1\n",
    "                uplefty = gt_split[gt_index]\n",
    "                uplefty = int(uplefty[1:-1])\n",
    "                upleftx = gt_split[gt_index+1]\n",
    "                upleftx = int(upleftx[0:-1])\n",
    "                downrighty = gt_split[gt_index+2]\n",
    "                downrighty = int(downrighty[0:-1])\n",
    "                downrightx = gt_split[gt_index+3]\n",
    "                downrightx = int(downrightx[0:-2])\n",
    "                \n",
    "                downrighty = np.min([downrighty+dt_d, h-1])\n",
    "                downrightx = np.min([downrightx+dt_d, w-1])\n",
    "                uplefty = np.max([uplefty-dt_d,0])\n",
    "                upleftx = np.max([upleftx-dt_d,0])\n",
    "                cv2.rectangle(color_gt,(np.int16(upleftx),np.int16(uplefty)),(np.int16(downrightx),np.int16(downrighty)), (255,0,0),1)\n",
    "                \n",
    "\n",
    "                gt_mask[uplefty:downrighty, upleftx:downrightx] = bbox_index#255\n",
    "                gt_ft_maske[uplefty:downrighty, upleftx:downrightx] = 255\n",
    "                centers.append([(upleftx+downrightx)/2,(uplefty+downrighty)/2])\n",
    "                gt_index += 4\n",
    "            oriImage = color_gt.copy() \n",
    "            oriImage_1 = color_gt.copy()\n",
    "\n",
    "            \n",
    "            # extract feature points for previous frame gray = Xt-1. By using maskOut function, only keep features outside the pitot tube area\n",
    "            if pImg is None or frameidx % track_len == 0:\n",
    "                pImg = cv2.goodFeaturesToTrack(np.uint8(gray), **feature_params)\n",
    "                pImg = maskOut(blocks, pImg)\n",
    "\n",
    "            # compute onedirectional error Et-1 using backward transform to save computational time\n",
    "            if (frameidx) % detect_interval == 0:\n",
    "                weightedError,H_back,pImg = backgroundsubtraction(gray,prepreFrame, Xt,pImg,blocks,lamda, lk_params,use_ransac)\n",
    "            else:\n",
    "                H_back,pImg = backgroundMotion(gray,prepreFrame, Xt,pImg,blocks,lamda, lk_params, use_ransac)\n",
    "            \n",
    "            ###########################Part I.b Feature Extraction on Background Subtracted Image for Every Other 20 Frames###############################\n",
    "            #print('Start:', len(Dotft))\n",
    "            #start_time = time.time()\n",
    "\n",
    "            if len(Dotft)>0:\n",
    "                #print(frameidx, 'previous:', len(Dotft))\n",
    "                d1, d, p1, pPers, p0, st1, ft_mv, ft_app, gt_labels = generatePatches_MV_trackV1(Dotft, gray, Xt, H_back, lk_params_track_ori, radius,w, h, color, gt_ft_maske)\n",
    "                #print(frameidx, 'after:', len(Dotft))\n",
    "                score_mv = mvmodel.predict(ft_mv, batch_size = 1000000)\n",
    "                #print(\"--- %s seconds(deeplearning_motion) ---\" % (time.time() - start_time))\n",
    "                #start_time1 = time.time()\n",
    "                score_app = appmodel_track.predict(ft_app,batch_size= 2560)\n",
    "                #print(\"--- %s seconds(deeplearning_app) ---\" % (time.time() - start_time1))\n",
    "                \n",
    "                #start_time1 = time.time()\n",
    "                bifeature = np.hstack([score_app[:,0].reshape(-1,1),score_mv[:,0].reshape(-1,1)])                    \n",
    "                trst = combinemodel_track.predict(bifeature)                \n",
    "                #print(\"--- %s seconds(adaboost) ---\" % (time.time() - start_time1))\n",
    "                \n",
    "                #start_time1 = time.time()\n",
    "                #Dotft, indrm = prunddt(Dotft, Patchft)\n",
    "                Dotft,indrm = dotupdate(Dotft, Patchft)\n",
    "                #print(\"--- %s seconds(dotupdate) ---\" % (time.time() - start_time1))\n",
    "                oriImage = visDotft(oriImage, Dotft,w, h)\n",
    "                #start_time1 = time.time()\n",
    "                Dotft = dottrack_detect(Dotft, p1[indrm], pPers[indrm], trst[indrm], st1[indrm], d1[indrm], d[indrm], Patchft)#updatetr(p1, st1)\n",
    "                #Dotft = dottrack_detect(Dotft, p1, pPers, trst, st1, d1, d, Patchft)#updatetr(p1, st1)\n",
    "                \n",
    "                oriImage = visPtV1(oriImage, p0, st1, d1)\n",
    "                #print(\"--- %s seconds(dottrack_detect) ---\" % (time.time() - start_time1))\n",
    "                #start_time1 = time.time()\n",
    "                \n",
    "                #print(\"--- %s seconds(visPtV1) ---\" % (time.time() - start_time1))\n",
    "            #print(\"--- %s seconds(updateDot) ---\" % (time.time() - start_time))\n",
    "            #print('Midd1:', len(Dotft))\n",
    "            #start_time = time.time()\n",
    "            if len(Patchft)>0:\n",
    "                #print('hahha')\n",
    "                #print('before:', len(Patchft))\n",
    "                oriImage = visDetect_Kalman(Patchft, oriImage, radius, w, h)\n",
    "                #print(\"--- %s seconds(visDetect_Kalman) ---\" % (time.time() - start_time))\n",
    "                #start_time1 = time.time()\n",
    "                outputFeature = writeDetect(outputFeature, radius, Patchft, w, h)\n",
    "                #print(\"--- %s seconds(writeDetect) ---\" % (time.time() - start_time1))\n",
    "                #start_time2 = time.time()\n",
    "                Patchft = patch_KalmanTracking(Dotft, Patchft, H_back, w, h)\n",
    "                #print(\"--- %s seconds(patch_KalmanTracking) ---\" % (time.time() - start_time2))\n",
    "                \n",
    "                #print('after:', len(Patchft))\n",
    "            #print('Midd2:', len(Patchft))\n",
    "            #print(\"--- %s seconds(updatePatch) ---\" % (time.time() - start_time))\n",
    "            #start_time = time.time()\n",
    "            if (frameidx) % detect_interval == 0:\n",
    "                #p_pos, p_pos_errImg, p_neg, p_neg_errImg, p_pos_gt, p_pos_gt_errImg, hit,ftNo,FAno, vis_points = generatePatches(frameidx, gray, Xt, weightedError, centers, H_back, feature_params_track, feature_params_track_oriImg, lk_params_track, radius, color, future_color, gt_mask, oriImage)\n",
    "                mv, detectedPatches, errorPatches, gt_labels, detectedLocs, curLocslll, hit, ftNo, FAno = Extract_Patch(frameidx, gray, Xt, weightedError, centers, H_back, feature_params_track, feature_params_track_oriImg, lk_params_track, radius, color, future_color, gt_mask, oriImage)\n",
    "                \n",
    "                #errorPatches, detectedPatches, detectedLocs, Locs_next, Locs_pers, d_match = generatePatches_online(gray, Xt, weightedError, feature_params_track, color, radius,lk_params_track,H_back)\n",
    "                if mv.shape[0]>0:\n",
    "                    errorPatches = errorPatches[:,:,:, newaxis]\n",
    "                    mv = np.hstack([mv[:,4:6],mv[:,10:]])\n",
    "                    #print(detectedPatches.shape, errorPatches.shape,errorPatche_.shape)\n",
    "                    data_np_test = np.concatenate([detectedPatches/255.0 ,errorPatches/255.0,errorPatches/255.0,errorPatches/255.0], axis=3)#errorPatches/255.0\n",
    "                    test_output_app = appmodel.predict(data_np_test,batch_size= 2560)\n",
    "                    #pred_y = np.argmax(test_output, 1)\n",
    "                    \n",
    "                    test_output_mv = mvmodel.predict(mv, batch_size = 1000000)\n",
    "                    \n",
    "                    mvmafeature = np.hstack([test_output_app[:,0].reshape(-1,1),test_output_mv[:,0].reshape(-1,1)])\n",
    "                    \n",
    "                    dt_lable = combinemodel.predict(mvmafeature)                   \n",
    "                    \n",
    "                    oriImage = visPosPatch_Kalman(dt_lable, gt_labels, detectedLocs, oriImage, radius)\n",
    "                    #print('frameidx', frameidx)\n",
    "                    oriImage, Dotft, Patchft, maxPatchId = DetectOnX_V2(maxD, maxPatchId, oriImage, gray, Xt, lk_params_track_ori, H_back, detectedLocs, curLocslll, dt_lable, detectedPatches,feature_params_Detect, radius, Dotft, Patchft)\n",
    "\n",
    "\n",
    "            #print(\"--- %s seconds(newDetection) ---\" % (time.time() - start_time))\n",
    "            \n",
    "            #strat_time = time.time()\n",
    "            draw_str(oriImage, 20, 60, 'frame ID: %d' % (frameidx-1))\n",
    "            video_PosPatch.write(oriImage)\n",
    "            prepreFrame = Xtminus1.copy()\n",
    "            color = future_color.copy()\n",
    "            Xtminus1 = Xt.copy()\n",
    "            future_color_gt = future_color.copy()#cam_gt.read()[1]\n",
    "            f_txt.write(outputFeature+\"\\n\")\n",
    "            groundtruth = gt_text.readline()\n",
    "            outputFeature = \"time_layer: \"+ str(frameidx)+\" detections: \"\n",
    "            #print(\"--- %s seconds(writeout Results) ---\" % (time.time() - start_time))\n",
    "        video_PosPatch.release()\n",
    "        f_txt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0010378360748291016 seconds(deeplearning_motion) ---\n",
      "--- 0.0013594627380371094 seconds(deeplearning_app) ---\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0349118709564209 seconds(FeatureExtraction) ---\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.02722024917602539 seconds(FeatureExtraction) ---\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
